{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 초기 라이브러리 및 함수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "ENV_NAME = \"CartPole-v1\" # Game 이름 \n",
    "\n",
    "GAMMA = 0.95 # Discount Factor\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "MEMORY_SIZE = 1000000 #Replay memory최대 저장 개수\n",
    "BATCH_SIZE = 20 #memory에 history data가 20개까지 있을 때\n",
    "\n",
    "EXPLORATION_MAX = 1.0 #최대 exploration 값 \n",
    "EXPLORATION_MIN = 0.01 #최소 exploration 값\n",
    "EXPLORATION_DECAY = 0.995 #exploration 값이 작아지게 하기 위해서 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN network 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DQNSolver:\n",
    "\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        self.exploration_rate = EXPLORATION_MAX #초반에 Exploration으로 탐색을 하는 비율정의\n",
    "\n",
    "        self.action_space = action_space # Action Space :2\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE) # Replay Memory 정의: deque함수로 정의  \n",
    "        \n",
    "        ## Feed Foward Network 정의 \n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(24, input_shape=(observation_space,), activation=\"relu\"))\n",
    "        self.model.add(Dense(24, activation=\"relu\"))\n",
    "        self.model.add(Dense(self.action_space, activation=\"linear\"))\n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=LEARNING_RATE)) # loss function 및 optimizer 정의\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay memory 저의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        # agent의 state, action, reward, next_state, 게임이 끝났는지 등을 메모리 버퍼에 저장\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def act(self, state):\n",
    "        #exploration_rate가 초반에는 1.0 이지만, 학습을 거급하면서 점점 작아진다.\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            # Random으로 action을 선택\n",
    "            return random.randrange(self.action_space)\n",
    "        # agent가 직접 action을 선택\n",
    "        q_values = self.model.predict(state)\n",
    "        # 오른쪽, 왼쪽으로 예측한 값 중에 확률값이 높은것을 선택해서 return 한다\n",
    "        return np.argmax(q_values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 Process 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def experience_replay(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        batch = random.sample(self.memory, BATCH_SIZE)# 버퍼에서 batch size만큼 rand\n",
    "        for state, action, reward, state_next, terminal in batch:\n",
    "            q_update = reward #현제  state와 action을 했을 때 reward\n",
    "            if not terminal:# 이 Action을 했을 때 게임이 끝나지 않았다면\n",
    "                q_update = (reward + GAMMA * np.amax(self.model.predict(state_next)[0])) # q_update -> reward를 계산 \n",
    "            q_values = self.model.predict(state)\n",
    "            q_values[0][action] = q_update # 기존 action의 q_value값 다시 계산 : 게임이 끝났다면 기존 reward를 끝나지 않았다면 예측한 q_update 값\n",
    "            self.model.fit(state, q_values, verbose=0) #model 학습 \n",
    "        self.exploration_rate *= EXPLORATION_DECAY\n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)#exploration_rate 줄여나감"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main 실행 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
